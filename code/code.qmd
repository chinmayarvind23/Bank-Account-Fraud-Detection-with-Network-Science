---
title: "Bank Account Fraud Detection - COSC 421 Course Project"
output-file: index.html
format:
  html:
    theme: flatly
    css: styles/styles.css
    toc: true
    toc-location: left
---

#### Team Members: Chinmay Arvind, Dmitry Kostyukov, Jerry Fan, Rylan Millar

# Imports & Libraries Setup

```{r}
renv::snapshot()
source("../renv/activate.R")
renv::init()
renv::status()

required_packages <- c("rmarkdown", "tidyverse", "visNetwork", "AzureStor")

for (pkg in required_packages) {
  if (!requireNamespace(pkg, quietly = TRUE)) {
    install.packages(pkg, repos = "https://cloud.r-project.org")
  }
}
install.packages(c('rmarkdown', 'tidyverse', 'visNetwork', 'AzureStor'))
install.packages(c('readr', 'dplyr', 'httr'))
install.packages("igraph")
install.packages("dotenv")
install.packages("tidyverse")
renv::snapshot()
library(rmarkdown)
library(tidyverse)
library(visNetwork)
library(igraph)
library(AzureStor)
library(readr)
library(dplyr)
library(httr)
library(dotenv)
library(tidyverse)
library(purrr)
```

# Data loading

```{r}
load_dot_env(file = "C:/Users/dimdi/OneDrive/Documents/COSC 421/Project/Bank-Account-Fraud-Detection-with-Network-Science/data.env")
storage_container_url <- Sys.getenv("AZURE_STORAGE_SAS_URL")
if (storage_container_url == "") {
  stop("SAS URL not found. Please set the 'AZURE_STORAGE_SAS_URL' environment variable.")
}

bankfraud_data <- read_csv(storage_container_url)
bankfraud_data
```

# Data cleaning, and PCA

```{r}
colSums(is.na(bankfraud_data))
```

No missing values, so we can proceed with the rest of the data cleaning.

```{r}
summary(bankfraud_data)
str(bankfraud_data)
```

```{r}
bankfraud_data <- bankfraud_data[!duplicated(bankfraud_data), ]
bankfraud_data
```

No duplicates identified, so we can proceed with PCA and feature engineering. Removing the Device OS columns and the source columns as they are not helpful predictors in predicting bank fraud. The character data type columns are converted to numeric types for making PCA possible.

```{r}
bankfraud_data <- bankfraud_data[, -c(26, 28)]
bankfraud_data <- bankfraud_data %>% mutate(across(where(is.character), ~ as.numeric(as.factor(.))))
bankfraud_data
```

Printing a summary of the dataset so far.

```{r}
str(bankfraud_data)
```

Removing columns with 0 variance in them, as they will not contribute to predicting fraud in any way.

```{r}
not_required_cols <- sapply(bankfraud_data, function(x) var(x, na.rm = TRUE) == 0)
print("Columns with zero variance:")
print(names(bankfraud_data)[not_required_cols])
bankfraud_data <- bankfraud_data[, !not_required_cols]
print("Remaining columns after removing zero variance columns:")
print(names(bankfraud_data))
```

Printing a summary of the PCA performed after scaling the data, which shows the number of principal components that explain x% of the variance in the dataset along with charts that indicate the same.

```{r}
pcaCharts <- function(x) {
    x.var <- x$sdev ^ 2
    x.pvar <- x.var/sum(x.var)
    print("proportions of variance:")
    print(x.pvar)
    
    par(mfrow=c(2,2))
    plot(x.pvar,xlab="Principal component", ylab="Proportion of variance explained", ylim=c(0,1), type='b')
    plot(cumsum(x.pvar),xlab="Principal component", ylab="Cumulative Proportion of variance explained", ylim=c(0,1), type='b')
    screeplot(x)
    screeplot(x,type="l")
    par(mfrow=c(1,1))
}

bankfraud_data_pca <- prcomp(scale(bankfraud_data),center = TRUE)
print(bankfraud_data_pca)
summary(bankfraud_data_pca)
pcaCharts(bankfraud_data_pca)
```

Printing the covariance and correlation matrices of the data.

```{r}
covariancematrix <- cov(bankfraud_data)
covariancematrix

corellationmatrix = cor(bankfraud_data)
corellationmatrix
```

Shows the data points based on their fraud classification status on a biplot showing how much each feature contributes to the first 2 principal components.

```{r}
install.packages("ggplot2")
install.packages("ggfortify")
library(ggplot2)
library(ggfortify)
autoplot(bankfraud_data_pca, data = bankfraud_data, colour = 'fraud_bool', loadings = TRUE, loadings.label = TRUE, loadings.label.size = 3)
```

Setting an absolute threshold of 0.5 for selecting the impact of features on the principal components.

```{r}
pca_loadings <- bankfraud_data_pca$rotation[, 1:24]
threshold <- 0.5
selected_features <- unique(c(
  rownames(pca_loadings)[abs(pca_loadings[,1]) > threshold],
  rownames(pca_loadings)[abs(pca_loadings[,2]) > threshold],
  rownames(pca_loadings)[abs(pca_loadings[,3]) > threshold],
  rownames(pca_loadings)[abs(pca_loadings[,4]) > threshold],
  rownames(pca_loadings)[abs(pca_loadings[,5]) > threshold],
  rownames(pca_loadings)[abs(pca_loadings[,6]) > threshold],
  rownames(pca_loadings)[abs(pca_loadings[,7]) > threshold],
  rownames(pca_loadings)[abs(pca_loadings[,8]) > threshold],
  rownames(pca_loadings)[abs(pca_loadings[,9]) > threshold],
  rownames(pca_loadings)[abs(pca_loadings[,10]) > threshold],
  rownames(pca_loadings)[abs(pca_loadings[,11]) > threshold],
  rownames(pca_loadings)[abs(pca_loadings[,12]) > threshold],
  rownames(pca_loadings)[abs(pca_loadings[,13]) > threshold],
  rownames(pca_loadings)[abs(pca_loadings[,14]) > threshold],
  rownames(pca_loadings)[abs(pca_loadings[,15]) > threshold],
  rownames(pca_loadings)[abs(pca_loadings[,16]) > threshold],
  rownames(pca_loadings)[abs(pca_loadings[,17]) > threshold],
  rownames(pca_loadings)[abs(pca_loadings[,18]) > threshold],
  rownames(pca_loadings)[abs(pca_loadings[,19]) > threshold],
  rownames(pca_loadings)[abs(pca_loadings[,20]) > threshold],
  rownames(pca_loadings)[abs(pca_loadings[,21]) > threshold],
  rownames(pca_loadings)[abs(pca_loadings[,22]) > threshold],
  rownames(pca_loadings)[abs(pca_loadings[,23]) > threshold],
  rownames(pca_loadings)[abs(pca_loadings[,24]) > threshold]))

print(selected_features)
```

Prting the rotation matrix of the PCA that indicates the magnitude and direction of contribution of a feature to various principal components.

```{r}
bankfraud_data_pca$rotation
```

Final dataset with most relevant features for predicting fraud.

Rows with missing (-1) values in the bank_months_count and current_address_months_count are removed as these are considered missing values. The days_since_request column is removed due to having unreliable data; the data is supposed to range from 0 to 78 days, however the majority of the entries are less than one. The negative values from velocity_6h are also removed as they are uninterpretable in the context of the velocity of applications.

```{r}
bankfraud_data <- bankfraud_data[, c(selected_features)]
bankfraud_data %>% relocate(fraud_bool)
bankfraud_data

str(bankfraud_data)

# Checking for inconsistent values:
range(bankfraud_data$bank_months_count)
range(bankfraud_data$current_address_months_count)
range(bankfraud_data$days_since_request)
# Removing -1 values
bankfraud_data = bankfraud_data[bankfraud_data$bank_months_count != -1, ]
bankfraud_data = bankfraud_data[bankfraud_data$current_address_months_count != -1, ]
# Dropping days_since_request column
bankfraud_data = subset(bankfraud_data, select = -c(days_since_request))
# Removing negative values from velocity_6h
bankfraud_data = bankfraud_data[bankfraud_data$velocity_6h >= 0, ]

nrow(bankfraud_data)

```

# Feature Engineering

```{r}
str(bankfraud_data)

# Checking ranges:
range(bankfraud_data$payment_type)
range(bankfraud_data$bank_months_count)
range(bankfraud_data$bank_branch_count_8w)
range(bankfraud_data$zip_count_4w)
range(bankfraud_data$name_email_similarity)
range(bankfraud_data$bank_months_count)
range(bankfraud_data$housing_status)
range(bankfraud_data$velocity_6h)
range(bankfraud_data$current_address_months_count)

# Adding node_id column to identify vertices:

bankfraud_data$node_id = seq.int(nrow(bankfraud_data))
bankfraud_data = bankfraud_data %>% relocate(node_id)

# Creating Bins for bank_branch_count_8w, zip_count_4w, name_email_similarity, bank_months_count, velocity_6h, current_address_months_count:

bin_count = 5

binned_bankfraud_data = bankfraud_data

cols_to_bin = c("bank_branch_count_8w", "zip_count_4w", "name_email_similarity", "bank_months_count", "velocity_6h", "current_address_months_count")

binned_bankfraud_data[cols_to_bin] = data.frame(lapply(binned_bankfraud_data[cols_to_bin], function(x) cut(x, breaks = bin_count, labels=FALSE)))

str(binned_bankfraud_data)

# Creating binary combination column:

binary_columns = c("keep_alive_session", "foreign_request", "email_is_free", "fraud_bool", "phone_home_valid")

binned_bankfraud_data$binary_combination_value = apply(binned_bankfraud_data[binary_columns], 1, function(x) {paste(as.character(x), collapse = "")})

str(binned_bankfraud_data)

# Creating sample:

sample_size = 50
data_sample = sample_n(binned_bankfraud_data, sample_size)

str(data_sample)

# Creating edge list:
  
edges_list <- expand.grid(row_id_1 = seq_len(nrow(data_sample)), row_id_2 = seq_len(nrow(data_sample))) %>%
  filter(row_id_1 < row_id_2) %>%  # Remove duplicates and self-loops
  rowwise() %>%
  mutate(
    edge_type = list(names(data_sample)[-c(1,3:6,13)][
      sapply(names(data_sample)[-c(1,3:6,13)], function(col) {
        data_sample[[col]][row_id_1] == data_sample[[col]][row_id_2]
      })
    ])
  ) %>%
  filter(length(edge_type) > 0) %>%
  unnest(edge_type) %>%
  ungroup() %>%
  mutate(
    node_id_1 = data_sample$node_id[row_id_1],
    node_id_2 = data_sample$node_id[row_id_2]
  ) %>%
  select(node_id_1, node_id_2, edge_type)

print(edges_list)
nrow(edges_list)
unique(edges_list$edge_type)

# Plotting the Data:

graph <- graph_from_data_frame(d = edges_list, vertices = data_sample, directed = FALSE)

edge_type_colors <- c("payment_type" = "green", "bank_branch_count_8w" = "blue", "name_email_similarity" = "yellow",
                       "zip_count_4w" = "cyan", "velocity_6h" = "blueviolet", "current_address_months_count" = "gold4",
                       "bank_months_count" = "darkseagreen1", "housing_status" = "darkorange1", "binary_combination_value" = "black")
E(graph)$color <- edge_type_colors[E(graph)$edge_type]

plot(graph, vertex.size = 20, edge.width = 1, edge.color = E(graph)$color)

```