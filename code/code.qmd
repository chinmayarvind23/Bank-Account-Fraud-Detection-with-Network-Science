---
title: "Bank Account Fraud Detection - COSC 421 Course Project"
output-file: index.html
format:
  html:
    theme: flatly
    css: styles/styles.css
    toc: true
    toc-location: left
---

#### Team Members: Chinmay Arvind, Dmitry Kostyukov, Jerry Fan, Rylan Millar

# Imports & Libraries Setup

```{r}
renv::snapshot()
source("../renv/activate.R")
renv::init()
renv::status()

required_packages <- c("rmarkdown", "tidyverse", "visNetwork", "AzureStor")

for (pkg in required_packages) {
  if (!requireNamespace(pkg, quietly = TRUE)) {
    install.packages(pkg, repos = "https://cloud.r-project.org")
  }
}
install.packages(c('rmarkdown', 'tidyverse', 'visNetwork', 'AzureStor'))
install.packages(c('readr', 'dplyr', 'httr'))
install.packages("igraph")
install.packages("dotenv")
install.packages("tidyverse")
install.packages("dotenv")
install.packages("lsa")
renv::snapshot()
library(lsa)
library(rmarkdown)
library(tidyverse)
library(visNetwork)
library(igraph)
library(AzureStor)
library(readr)
library(dplyr)
library(httr)
library(dotenv)
library(tidyverse)
library(purrr)
library(dotenv)
exists("load_dot_env", where = asNamespace("dotenv"))
```

# Data loading

```{r}
load_dot_env(file = "C:/Users/jcfan/OneDrive/Desktop/Bank-Account-Fraud-Detection-with-Network-Science/data.env")
storage_container_url <- Sys.getenv("AZURE_STORAGE_SAS_URL")
if (storage_container_url == "") {
  stop("SAS URL not found. Please set the 'AZURE_STORAGE_SAS_URL' environment variable.")
}

#bankfraud_data <- read_csv(storage_container_url)
bankfraud_data <- read_csv(file = "C:/Users/jcfan/OneDrive/Desktop/Bank-Account-Fraud-Detection-with-Network-Science/data/Base.csv")
bankfraud_data
```

# Data cleaning, and PCA

```{r}
colSums(is.na(bankfraud_data))
```

No missing values, so we can proceed with the rest of the data cleaning.

```{r}
summary(bankfraud_data)
str(bankfraud_data)
```

```{r}
bankfraud_data <- bankfraud_data[!duplicated(bankfraud_data), ]
bankfraud_data
```

No duplicates identified, so we can proceed with PCA and feature engineering. Removing the Device OS columns and the source columns as they are not helpful predictors in predicting bank fraud. The character data type columns are converted to numeric types for making PCA possible.

```{r}
bankfraud_data <- bankfraud_data[, -c(26, 28)]
bankfraud_data <- bankfraud_data %>% mutate(across(where(is.character), ~ as.numeric(as.factor(.))))
bankfraud_data
```

Printing a summary of the dataset so far.

```{r}
str(bankfraud_data)
```

Removing columns with 0 variance in them, as they will not contribute to predicting fraud in any way.

```{r}
not_required_cols <- sapply(bankfraud_data, function(x) var(x, na.rm = TRUE) == 0)
print("Columns with zero variance:")
print(names(bankfraud_data)[not_required_cols])
bankfraud_data <- bankfraud_data[, !not_required_cols]
print("Remaining columns after removing zero variance columns:")
print(names(bankfraud_data))
```

Printing a summary of the PCA performed after scaling the data, which shows the number of principal components that explain x% of the variance in the dataset along with charts that indicate the same.

```{r}
pcaCharts <- function(x) {
    x.var <- x$sdev ^ 2
    x.pvar <- x.var/sum(x.var)
    print("proportions of variance:")
    print(x.pvar)
    
    par(mfrow=c(2,2))
    plot(x.pvar,xlab="Principal component", ylab="Proportion of variance explained", ylim=c(0,1), type='b')
    plot(cumsum(x.pvar),xlab="Principal component", ylab="Cumulative Proportion of variance explained", ylim=c(0,1), type='b')
    screeplot(x)
    screeplot(x,type="l")
    par(mfrow=c(1,1))
}

bankfraud_data_pca <- prcomp(scale(bankfraud_data),center = TRUE)
print(bankfraud_data_pca)
summary(bankfraud_data_pca)
pcaCharts(bankfraud_data_pca)
```

Printing the covariance and correlation matrices of the data.

```{r}
covariancematrix <- cov(bankfraud_data)
covariancematrix

corellationmatrix = cor(bankfraud_data)
corellationmatrix
```

Shows the data points based on their fraud classification status on a biplot showing how much each feature contributes to the first 2 principal components.

```{r}
install.packages("ggplot2")
install.packages("ggfortify")
library(ggplot2)
library(ggfortify)
autoplot(bankfraud_data_pca, data = bankfraud_data, colour = 'fraud_bool', loadings = TRUE, loadings.label = TRUE, loadings.label.size = 3)
```

Setting an absolute threshold of 0.5 for selecting the impact of features on the principal components.

```{r}
pca_loadings <- bankfraud_data_pca$rotation[, 1:24]
threshold <- 0.5
selected_features <- unique(c(
  rownames(pca_loadings)[abs(pca_loadings[,1]) > threshold],
  rownames(pca_loadings)[abs(pca_loadings[,2]) > threshold],
  rownames(pca_loadings)[abs(pca_loadings[,3]) > threshold],
  rownames(pca_loadings)[abs(pca_loadings[,4]) > threshold],
  rownames(pca_loadings)[abs(pca_loadings[,5]) > threshold],
  rownames(pca_loadings)[abs(pca_loadings[,6]) > threshold],
  rownames(pca_loadings)[abs(pca_loadings[,7]) > threshold],
  rownames(pca_loadings)[abs(pca_loadings[,8]) > threshold],
  rownames(pca_loadings)[abs(pca_loadings[,9]) > threshold],
  rownames(pca_loadings)[abs(pca_loadings[,10]) > threshold],
  rownames(pca_loadings)[abs(pca_loadings[,11]) > threshold],
  rownames(pca_loadings)[abs(pca_loadings[,12]) > threshold],
  rownames(pca_loadings)[abs(pca_loadings[,13]) > threshold],
  rownames(pca_loadings)[abs(pca_loadings[,14]) > threshold],
  rownames(pca_loadings)[abs(pca_loadings[,15]) > threshold],
  rownames(pca_loadings)[abs(pca_loadings[,16]) > threshold],
  rownames(pca_loadings)[abs(pca_loadings[,17]) > threshold],
  rownames(pca_loadings)[abs(pca_loadings[,18]) > threshold],
  rownames(pca_loadings)[abs(pca_loadings[,19]) > threshold],
  rownames(pca_loadings)[abs(pca_loadings[,20]) > threshold],
  rownames(pca_loadings)[abs(pca_loadings[,21]) > threshold],
  rownames(pca_loadings)[abs(pca_loadings[,22]) > threshold],
  rownames(pca_loadings)[abs(pca_loadings[,23]) > threshold],
  rownames(pca_loadings)[abs(pca_loadings[,24]) > threshold]))

print(selected_features)
```

Prting the rotation matrix of the PCA that indicates the magnitude and direction of contribution of a feature to various principal components.

```{r}
bankfraud_data_pca$rotation
```

Final dataset with most relevant features for predicting fraud.

Rows with missing (-1) values in the bank_months_count and current_address_months_count are removed as these are considered missing values. The days_since_request column is removed due to having unreliable data; the data is supposed to range from 0 to 78 days, however the majority of the entries are less than one. The negative values from velocity_6h are also removed as they are uninterpretable in the context of the velocity of applications.

```{r}
bankfraud_data <- bankfraud_data[, c(selected_features)]
bankfraud_data %>% relocate(fraud_bool)
bankfraud_data

str(bankfraud_data)

# Checking for inconsistent values:
range(bankfraud_data$bank_months_count)
range(bankfraud_data$current_address_months_count)
range(bankfraud_data$days_since_request)
# Removing -1 values
bankfraud_data = bankfraud_data[bankfraud_data$bank_months_count != -1, ]
bankfraud_data = bankfraud_data[bankfraud_data$current_address_months_count != -1, ]
# Dropping days_since_request column
bankfraud_data = subset(bankfraud_data, select = -c(days_since_request))
# Removing negative values from velocity_6h
bankfraud_data = bankfraud_data[bankfraud_data$velocity_6h >= 0, ]

nrow(bankfraud_data)

```

# Feature Engineering

```{r}
str(bankfraud_data)

# Checking ranges:
range(bankfraud_data$payment_type)
range(bankfraud_data$bank_months_count)
range(bankfraud_data$bank_branch_count_8w)
range(bankfraud_data$zip_count_4w)
range(bankfraud_data$name_email_similarity)
range(bankfraud_data$bank_months_count)
range(bankfraud_data$housing_status)
range(bankfraud_data$velocity_6h)
range(bankfraud_data$current_address_months_count)

# Adding node_id column to identify vertices:

bankfraud_data$node_id = seq.int(nrow(bankfraud_data))
bankfraud_data = bankfraud_data %>% relocate(node_id)

# Creating Bins for bank_branch_count_8w, zip_count_4w, name_email_similarity, bank_months_count, velocity_6h, current_address_months_count:

bin_count = 10

binned_bankfraud_data = bankfraud_data

cols_to_bin = c("bank_branch_count_8w", "zip_count_4w", "name_email_similarity", "bank_months_count", "velocity_6h", "current_address_months_count")

binned_bankfraud_data[cols_to_bin] = data.frame(lapply(binned_bankfraud_data[cols_to_bin], function(x) cut(x, breaks = bin_count, labels=FALSE)))

str(binned_bankfraud_data)

# Creating binary combination column:

binary_columns = c("keep_alive_session", "foreign_request", "email_is_free", "fraud_bool", "phone_home_valid")

binned_bankfraud_data$binary_combination_value = apply(binned_bankfraud_data[binary_columns], 1, function(x) {paste(as.character(x), collapse = "")})

str(binned_bankfraud_data)

#Using stratified sampling to ensure same proportion of fraudulent nodes:
sample_size = 100
sample_size_fraud = sample_size*0.01
sample_size_legit = sample_size*0.99

sample_fraud = binned_bankfraud_data %>% filter(fraud_bool == 1) %>% slice_sample(n = sample_size_fraud)
sample_legit = binned_bankfraud_data %>% filter(fraud_bool == 0) %>% slice_sample(n = sample_size_legit)

data_sample = bind_rows(sample_fraud, sample_legit)

str(data_sample)

# Creating edge list:
create_edges <- function(data_sample) {
  start.time <- Sys.time()
  edges_list <- expand.grid(row_id_1 = seq_len(nrow(data_sample)), row_id_2 = seq_len(nrow(data_sample))) %>%
    filter(row_id_1 < row_id_2) %>%  # Remove duplicates and self-loops
    rowwise() %>%
    mutate(
      edge_type = list(names(data_sample)[-c(1,3:6,13)][
        sapply(names(data_sample)[-c(1,3:6,13)], function(col) {
          data_sample[[col]][row_id_1] == data_sample[[col]][row_id_2]
        })
      ])
    ) %>%
    filter(length(edge_type) > 0) %>%
    unnest(edge_type) %>%
    ungroup() %>%
    mutate(
      node_id_1 = data_sample$node_id[row_id_1],
      node_id_2 = data_sample$node_id[row_id_2]
    ) %>%
    select(node_id_1, node_id_2, edge_type)
  end.time <- Sys.time()
  time.taken <- end.time - start.time
  print(time.taken)
  edges_list
}

edges_list = create_edges(data_sample)
print(edges_list)
nrow(edges_list)
unique(edges_list$edge_type)

# Plotting the Data:

graph <- graph_from_data_frame(d = edges_list, vertices = data_sample, directed = FALSE)

edge_type_colors <- c("payment_type" = "green", "bank_branch_count_8w" = "blue", "name_email_similarity" = "yellow",
                       "zip_count_4w" = "cyan", "velocity_6h" = "blueviolet", "current_address_months_count" = "gold4",
                       "bank_months_count" = "darkseagreen1", "housing_status" = "darkorange1", "binary_combination_value" = "black")
E(graph)$color <- edge_type_colors[E(graph)$edge_type]

par(mfrow=c(1,1), mar=c(0,0,0,0))
plot(graph, vertex.size = 10, edge.width = 2, edge.color = E(graph)$color)

```

Reasoning for sampling:

Due to the lengthy computation time of our edge-generating function, we have chosen to use samples of our full data to analyze the network. A one-thousand node sample takes approximately one minute to generate edges, a two-thousand sample take around ten minutes. This increase in time makes it unfeasible to use larger samples, let alone the entire data set of around 700 thousand nodes. Therefore, we will keep our sample sizes in the 1000 to 2000 range.

The percentage of fraudulent nodes in the full data set is approximately 1%. To ensure this proportion is consistent in our sample we use stratified sampling. Stratified sampling samples 1% of the data from a strata of only fraudulent nodes and the other 99% of the data from a strata of only the non-fraudulent nodes.

Prior to using a sample in our project, we ensure that the sample is representative of the entire data set by applying the Chi-Squared test to our sample. The Chi-Squared test will determine if there is a significant difference between the values in the columns of the sample and in the full data set. The Chi-Squared test generates p-values for each column in a sample. A p-value of less than 0.05 will signal a significant difference between the sample and the full data set for that particular column. By conducting the Chi-Squared test on a given sample we ensure that it is sufficiently representative of the entire data set.

```{r}
# Percentage of fraudulent nodes in the data frame:
fraud_percentage = (sum(bankfraud_data$fraud_bool == 1)/nrow(bankfraud_data))*100
fraud_percentage

str(data_sample)
str(binned_bankfraud_data)

# Factor data for chi_Squared test:

data_sample$payment_type = as.factor(data_sample$payment_type)
data_sample$bank_branch_count_8w = as.factor(data_sample$bank_branch_count_8w)
data_sample$zip_count_4w = as.factor(data_sample$zip_count_4w)
data_sample$name_email_similarity = as.factor(data_sample$name_email_similarity)
data_sample$bank_months_count = as.factor(data_sample$bank_months_count)
data_sample$housing_status = as.factor(data_sample$housing_status)
data_sample$velocity_6h = as.factor(data_sample$velocity_6h)
data_sample$current_address_months_count = as.factor(data_sample$current_address_months_count)

binned_bankfraud_data$payment_type = as.factor(binned_bankfraud_data$payment_type)
binned_bankfraud_data$bank_branch_count_8w = as.factor(binned_bankfraud_data$bank_branch_count_8w)
binned_bankfraud_data$zip_count_4w = as.factor(binned_bankfraud_data$zip_count_4w)
binned_bankfraud_data$name_email_similarity = as.factor(binned_bankfraud_data$name_email_similarity)
binned_bankfraud_data$bank_months_count = as.factor(binned_bankfraud_data$bank_months_count)
binned_bankfraud_data$housing_status = as.factor(binned_bankfraud_data$housing_status)
binned_bankfraud_data$velocity_6h = as.factor(binned_bankfraud_data$velocity_6h)
binned_bankfraud_data$current_address_months_count = as.factor(binned_bankfraud_data$current_address_months_count)

# Chi-Squared test:

chi_squared_results = list()

for (col in names(data_sample)[-c(1,3:6,13)]) {
  population_counts = table(binned_bankfraud_data[[col]])
  sample_counts = table(data_sample[[col]])
  
  all_levels = names(population_counts)
  sample_counts = sample_counts[all_levels]
  
  sample_counts[is.na(sample_counts)] = 0
  
  chi_test = chisq.test(x = sample_counts, p = population_counts/sum(population_counts))
  
  chi_squared_results[[col]] = chi_test$p.value
}

chi_squared_df = data.frame(Column = names(chi_squared_results), P_Value = unlist(chi_squared_results))

print(chi_squared_df)

```

# RQ 3.

What was the average profile of a fraudulent customer?

```{r}
print(nrow(fraud_transactions))
print(head(fraud_transactions))

```

```{r}
# Filter fraudulent transactions
fraud_transactions <- bankfraud_data %>% filter(fraud_bool == 1)
num_fraud_transactions <- nrow(fraud_transactions)

# Display number of fraudulent transactions
print(paste("Total number of fraudulent transactions:", num_fraud_transactions))

calculate_mode <- function(x) {
  ux <- unique(x)
  ux[which.max(tabulate(match(x, ux)))]
}

# Calculate the average profile
typical_fraud_node <- fraud_transactions %>%
  summarise(
    payment_type = calculate_mode(payment_type),
    keep_alive_session = calculate_mode(keep_alive_session),
    foreign_request = calculate_mode(foreign_request),
    email_is_free = calculate_mode(email_is_free),
    bank_branch_count_8w = mean(bank_branch_count_8w, na.rm = TRUE),
    zip_count_4w = mean(zip_count_4w, na.rm = TRUE),
    name_email_similarity = mean(name_email_similarity, na.rm = TRUE),
    bank_months_count = mean(bank_months_count, na.rm = TRUE),
    housing_status = calculate_mode(housing_status),
    velocity_6h = mean(velocity_6h, na.rm = TRUE),
    phone_home_valid = calculate_mode(phone_home_valid),
    current_address_months_count = mean(current_address_months_count, na.rm = TRUE)
  )

typical_fraud_node <- as.list(typical_fraud_node)

print("Average Profile of a Fraudulent Node:")
print(typical_fraud_node)


```

```{r}
calculate_similarity <- function(node, typical_fraud_node) {
  similarity_score <- sum(
    node$payment_type == typical_node$payment_type,
    node$keep_alive_session == typical_node$keep_alive_session,
    node$foreign_request == typical_node$foreign_request,
    node$email_is_free == typical_node$email_is_free,
    abs(node$bank_branch_count_8w - typical_node$bank_branch_count_8w) < 2,
    abs(node$zip_count_4w - typical_node$zip_count_4w) < 2,
    abs(node$name_email_similarity - typical_node$name_email_similarity) < 0.1,
    abs(node$bank_months_count - typical_node$bank_months_count) < 6,
    node$housing_status == typical_node$housing_status,
    abs(node$velocity_6h - typical_node$velocity_6h) < 0.5,
    node$phone_home_valid == typical_node$phone_home_valid,
    abs(node$current_address_months_count - typical_node$current_address_months_count) < 12
  )
  return(similarity_score)
}

# Create edges based on similarity to the typical node
similarity_threshold <- 8
typical_node_id <- "TypicalNode"
edges_list <- data.frame(node_id_1 = character(), node_id_2 = character(), similarity_score = numeric())

for (i in 1:nrow(fraud_transactions)) {
  node <- fraud_transactions[i, ]
  similarity_score <- calculate_similarity(node, typical_fraud_node)
  
  if (similarity_score >= similarity_threshold) {
    edges_list <- rbind(edges_list, data.frame(
      node_id_1 = typical_node_id,
      node_id_2 = node$node_id,
      similarity_score = similarity_score
    ))
  }
}

typical_node_row <- data.frame(
  node_id = typical_node_id,
  payment_type = typical_fraud_node$payment_type,
  keep_alive_session = typical_fraud_node$keep_alive_session,
  foreign_request = typical_fraud_node$foreign_request,
  email_is_free = typical_fraud_node$email_is_free,
  bank_branch_count_8w = typical_fraud_node$bank_branch_count_8w,
  zip_count_4w = typical_fraud_node$zip_count_4w,
  name_email_similarity = typical_fraud_node$name_email_similarity,
  bank_months_count = typical_fraud_node$bank_months_count,
  housing_status = typical_fraud_node$housing_status,
  velocity_6h = typical_fraud_node$velocity_6h,
  phone_home_valid = typical_fraud_node$phone_home_valid,
  current_address_months_count = typical_fraud_node$current_address_months_count,
  fraud_bool = 1
)
vertices <- rbind(fraud_transactions, typical_node_row)

graph_fraud <- graph_from_data_frame(d = edges_list, vertices = vertices, directed = FALSE)

E(graph_fraud)$weight <- edges_list$similarity_score

plot(graph_fraud, vertex.size = 1, vertex.label = NA, edge.width = E(graph_fraud)$weight / max(E(graph_fraud)$weight) * 2)

```

```{r}
# Define similarity thresholds
jaccard_threshold <- 0.5
cosine_threshold <- 0.7
weighted_threshold <- 0.6

# Columns for similarity calculations
binary_columns <- c("keep_alive_session", "foreign_request", "email_is_free", "phone_home_valid")
numeric_columns <- c("bank_branch_count_8w", "zip_count_4w", "name_email_similarity", 
                     "bank_months_count", "velocity_6h", "current_address_months_count")

# Define weights for weighted similarity
feature_weights <- c(
  bank_branch_count_8w = 0.2,
  zip_count_4w = 0.15,
  name_email_similarity = 0.25,
  bank_months_count = 0.1,
  velocity_6h = 0.2,
  current_address_months_count = 0.1
)

# Functions for calculating similarities
calculate_jaccard <- function(node, typical_fraud_node, binary_cols) {
  intersection <- sum(node[binary_cols] == typical_fraud_node[binary_cols])
  union <- length(binary_cols)
  return(intersection / union)
}

calculate_cosine <- function(node, typical_fraud_node, numeric_cols) {
  vec1 <- as.numeric(node[numeric_cols])
  vec2 <- as.numeric(typical_fraud_node[numeric_cols])
  return(cosine(vec1, vec2))
}

calculate_weighted <- function(node, typical_fraud_node, feature_weights) {
  weighted_sum <- sum(feature_weights * abs(as.numeric(node) - as.numeric(typical_fraud_node)))
  max_sum <- sum(feature_weights)
  return(1 - (weighted_sum / max_sum))  # Normalize to a similarity score
}

# Create empty edge lists
edges_jaccard <- data.frame(node_id_1 = character(), node_id_2 = character(), similarity = numeric())
edges_cosine <- data.frame(node_id_1 = character(), node_id_2 = character(), similarity = numeric())
edges_weighted <- data.frame(node_id_1 = character(), node_id_2 = character(), similarity = numeric())

# Iterate over fraudulent nodes
for (i in 1:nrow(fraud_transactions)) {
  node <- fraud_transactions[i, ]
  
  # Jaccard similarity
  jaccard_similarity <- calculate_jaccard(node, typical_fraud_node, binary_columns)
  if (jaccard_similarity >= jaccard_threshold) {
    edges_jaccard <- rbind(edges_jaccard, data.frame(
      node_id_1 = paste("TypicalNode"),
      node_id_2 = node$node_id,
      similarity = jaccard_similarity
    ))
  }
  
  # Cosine similarity
  cosine_similarity <- calculate_cosine(node, typical_fraud_node, numeric_columns)
  if (cosine_similarity >= cosine_threshold) {
    edges_cosine <- rbind(edges_cosine, data.frame(
      node_id_1 = paste("TypicalNode"),
      node_id_2 = node$node_id,
      similarity = cosine_similarity
    ))
  }
  
  # Weighted similarity
  weighted_similarity <- calculate_weighted(node[numeric_columns], typical_fraud_node[numeric_columns], feature_weights)
  if (weighted_similarity >= weighted_threshold) {
    edges_weighted <- rbind(edges_weighted, data.frame(
      node_id_1 = paste("TypicalNode"),
      node_id_2 = node$node_id,
      similarity = weighted_similarity
    ))
  }
}

# Create graphs for each similarity type
graph_jaccard <- graph_from_data_frame(d = edges_jaccard, vertices = fraud_transactions, directed = FALSE)
graph_cosine <- graph_from_data_frame(d = edges_cosine, vertices = fraud_transactions, directed = FALSE)
graph_weighted <- graph_from_data_frame(d = edges_weighted, vertices = fraud_transactions, directed = FALSE)

# Plot graphs
par(mfrow = c(1, 3))
plot(graph_jaccard, main = "Jaccard Similarity Graph", vertex.size = 1, edge.width = E(graph_jaccard)$similarity * 5)
plot(graph_cosine, main = "Cosine Similarity Graph", vertex.size = 1, edge.width = E(graph_cosine)$similarity * 5)
plot(graph_weighted, main = "Weighted Similarity Graph", vertex.size = 1, edge.width = E(graph_weighted)$similarity * 5)
```
