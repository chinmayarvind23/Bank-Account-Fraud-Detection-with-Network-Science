---
title: "Bank Account Fraud Detection - COSC 421 Course Project"
output-file: index.html
format:
  html:
    theme: flatly
    css: styles/styles.css
    toc: true
    toc-location: left
---



#### Team Members: Chinmay Arvind, Dmitry Kostyukov, Jerry Fan, Rylan Millar

# Imports & Libraries Setup



```{r}
renv::snapshot()
source("../renv/activate.R")
renv::init()
renv::status()

## required_packages <- c("rmarkdown", "tidyverse", "visNetwork", "AzureStor")


## for (pkg in required_packages) {
##  if (!requireNamespace(pkg, quietly = TRUE)) {
##    install.packages(pkg, repos = "https://cloud.r-project.org")
##  }
## }

install.packages(c('rmarkdown', 'tidyverse', 'visNetwork', 'AzureStor'))
install.packages(c('readr', 'dplyr', 'httr'))
install.packages("igraph")
install.packages("dotenv")
install.packages("tidyverse")
install.packages("purrr")
renv::snapshot()
library(rmarkdown)
library(tidyverse)
library(visNetwork)
library(igraph)
library(readr)
library(dplyr)
library(httr)
library(dotenv)
library(tidyverse)
library(purrr)
```



# Data loading



```{r}
load_dot_env(file = "C:/Users/Owner/MillarDrive/OneDrive/Bank-Account-Fraud-Detection-with-Network-Science/data.env")
storage_container_url <- Sys.getenv("AZURE_STORAGE_SAS_URL")
if (storage_container_url == "") {
  stop("SAS URL not found. Please set the 'AZURE_STORAGE_SAS_URL' environment variable.")
}

bankfraud_data <- read_csv("C:/Users/Owner/MillarDrive/OneDrive/Bank-Account-Fraud-Detection-with-Network-Science/Base.csv")
bankfraud_data
```



# Data cleaning, and PCA



```{r}
colSums(is.na(bankfraud_data))
```



No missing values, so we can proceed with the rest of the data cleaning.



```{r}
summary(bankfraud_data)
str(bankfraud_data)
```

```{r}
bankfraud_data <- bankfraud_data[!duplicated(bankfraud_data), ]
bankfraud_data
```



No duplicates identified, so we can proceed with PCA and feature engineering. Removing the Device OS columns and the source columns as they are not helpful predictors in predicting bank fraud. The character data type columns are converted to numeric types for making PCA possible.



```{r}
bankfraud_data <- bankfraud_data[, -c(26, 28)]
bankfraud_data <- bankfraud_data %>% mutate(across(where(is.character), ~ as.numeric(as.factor(.))))
bankfraud_data
```



Printing a summary of the dataset so far.



```{r}
str(bankfraud_data)
```



Removing columns with 0 variance in them, as they will not contribute to predicting fraud in any way.



```{r}
not_required_cols <- sapply(bankfraud_data, function(x) var(x, na.rm = TRUE) == 0)
print("Columns with zero variance:")
print(names(bankfraud_data)[not_required_cols])
bankfraud_data <- bankfraud_data[, !not_required_cols]
print("Remaining columns after removing zero variance columns:")
print(names(bankfraud_data))
```



Printing a summary of the PCA performed after scaling the data, which shows the number of principal components that explain x% of the variance in the dataset along with charts that indicate the same.



```{r}
pcaCharts <- function(x) {
    x.var <- x$sdev ^ 2
    x.pvar <- x.var/sum(x.var)
    print("proportions of variance:")
    print(x.pvar)
    
    par(mfrow=c(2,2))
    plot(x.pvar,xlab="Principal component", ylab="Proportion of variance explained", ylim=c(0,1), type='b')
    plot(cumsum(x.pvar),xlab="Principal component", ylab="Cumulative Proportion of variance explained", ylim=c(0,1), type='b')
    screeplot(x)
    screeplot(x,type="l")
    par(mfrow=c(1,1))
}

bankfraud_data_pca <- prcomp(scale(bankfraud_data),center = TRUE)
print(bankfraud_data_pca)
summary(bankfraud_data_pca)
pcaCharts(bankfraud_data_pca)
```



Printing the covariance and correlation matrices of the data.



```{r}
covariancematrix <- cov(bankfraud_data)
covariancematrix

corellationmatrix = cor(bankfraud_data)
corellationmatrix
```



Shows the data points based on their fraud classification status on a biplot showing how much each feature contributes to the first 2 principal components.



```{r}
install.packages("ggplot2")
install.packages("ggfortify")
library(ggplot2)
library(ggfortify)
autoplot(bankfraud_data_pca, data = bankfraud_data, colour = 'fraud_bool', loadings = TRUE, loadings.label = TRUE, loadings.label.size = 3)
```



Setting an absolute threshold of 0.5 for selecting the impact of features on the principal components.



```{r}
pca_loadings <- bankfraud_data_pca$rotation[, 1:24]
threshold <- 0.5
selected_features <- unique(c(
  rownames(pca_loadings)[abs(pca_loadings[,1]) > threshold],
  rownames(pca_loadings)[abs(pca_loadings[,2]) > threshold],
  rownames(pca_loadings)[abs(pca_loadings[,3]) > threshold],
  rownames(pca_loadings)[abs(pca_loadings[,4]) > threshold],
  rownames(pca_loadings)[abs(pca_loadings[,5]) > threshold],
  rownames(pca_loadings)[abs(pca_loadings[,6]) > threshold],
  rownames(pca_loadings)[abs(pca_loadings[,7]) > threshold],
  rownames(pca_loadings)[abs(pca_loadings[,8]) > threshold],
  rownames(pca_loadings)[abs(pca_loadings[,9]) > threshold],
  rownames(pca_loadings)[abs(pca_loadings[,10]) > threshold],
  rownames(pca_loadings)[abs(pca_loadings[,11]) > threshold],
  rownames(pca_loadings)[abs(pca_loadings[,12]) > threshold],
  rownames(pca_loadings)[abs(pca_loadings[,13]) > threshold],
  rownames(pca_loadings)[abs(pca_loadings[,14]) > threshold],
  rownames(pca_loadings)[abs(pca_loadings[,15]) > threshold],
  rownames(pca_loadings)[abs(pca_loadings[,16]) > threshold],
  rownames(pca_loadings)[abs(pca_loadings[,17]) > threshold],
  rownames(pca_loadings)[abs(pca_loadings[,18]) > threshold],
  rownames(pca_loadings)[abs(pca_loadings[,19]) > threshold],
  rownames(pca_loadings)[abs(pca_loadings[,20]) > threshold],
  rownames(pca_loadings)[abs(pca_loadings[,21]) > threshold],
  rownames(pca_loadings)[abs(pca_loadings[,22]) > threshold],
  rownames(pca_loadings)[abs(pca_loadings[,23]) > threshold],
  rownames(pca_loadings)[abs(pca_loadings[,24]) > threshold]))

print(selected_features)
```



Prting the rotation matrix of the PCA that indicates the magnitude and direction of contribution of a feature to various principal components.



```{r}
bankfraud_data_pca$rotation
```



# Final dataset with most relevant features for predicting fraud.

Rows with missing (-1) values in the bank_months_count and current_address_months_count are removed as these are considered missing values. The days_since_request column is removed due to having unreliable data - the data is supposed to range from 0 to 78 days, however the majority of the entries are less than one. The negative values from velocity_6h are also removed as they are uninterpretable in the context of the velocity of applications.



```{r}
bankfraud_data <- bankfraud_data[, c(selected_features)]
bankfraud_data %>% relocate(fraud_bool)
bankfraud_data

str(bankfraud_data)

# Checking for inconsistent values:
range(bankfraud_data$bank_months_count)
range(bankfraud_data$current_address_months_count)
range(bankfraud_data$days_since_request)

# Removing -1 values
bankfraud_data = bankfraud_data[bankfraud_data$bank_months_count != -1, ]
bankfraud_data = bankfraud_data[bankfraud_data$current_address_months_count != -1, ]

# Dropping days_since_request column
bankfraud_data = subset(bankfraud_data, select = -c(days_since_request))

# Removing negative values from velocity_6h
bankfraud_data = bankfraud_data[bankfraud_data$velocity_6h >= 0, ]
nrow(bankfraud_data)

```



# Feature Engineering

Checking data ranges, and enumerating records in the CSV file as nodes with Node IDs. We have taken a sample here to check if the graph creation is feasible before generating the graph with all of the nodes in it.



```{r}
str(bankfraud_data)

# Checking ranges:
range(bankfraud_data$payment_type)
range(bankfraud_data$bank_months_count)
range(bankfraud_data$bank_branch_count_8w)
range(bankfraud_data$zip_count_4w)
range(bankfraud_data$name_email_similarity)
range(bankfraud_data$bank_months_count)
range(bankfraud_data$housing_status)
range(bankfraud_data$velocity_6h)
range(bankfraud_data$current_address_months_count)

# Adding node_id column to identify vertices:
bankfraud_data$node_id = seq.int(nrow(bankfraud_data))
bankfraud_data = bankfraud_data %>% relocate(node_id)


#Rylan Millar - You can start changing your own stuff here
#
#
#


# Creating Bins for bank_branch_count_8w, zip_count_4w, name_email_similarity, bank_months_count, velocity_6h, current_address_months_count:
bin_count = 20
binned_bankfraud_data = bankfraud_data
cols_to_bin = c("bank_branch_count_8w", "zip_count_4w", "name_email_similarity", "bank_months_count", "velocity_6h", "current_address_months_count")
binned_bankfraud_data[cols_to_bin] = data.frame(lapply(binned_bankfraud_data[cols_to_bin], function(x) cut(x, breaks = bin_count, labels=FALSE)))
str(binned_bankfraud_data)

# Creating binary combination column:
binary_columns = c("keep_alive_session", "foreign_request", "email_is_free", "fraud_bool", "phone_home_valid")
binned_bankfraud_data$binary_combination_value = apply(binned_bankfraud_data[binary_columns], 1, function(x) {paste(as.character(x), collapse = "")})
str(binned_bankfraud_data)

# Creating sample:
sample_size = 200
data_sample = sample_n(binned_bankfraud_data, sample_size)
str(data_sample)

# Creating edge list:
edges_list <- expand.grid(row_id_1 = seq_len(nrow(data_sample)), row_id_2 = seq_len(nrow(data_sample))) %>%
  filter(row_id_1 < row_id_2) %>%  # Remove duplicates and self-loops
  rowwise() %>%
  mutate(
    edge_type = list(names(data_sample)[-c(1,3:6,13)][
      sapply(names(data_sample)[-c(1,3:6,13)], function(col) {
        data_sample[[col]][row_id_1] == data_sample[[col]][row_id_2]
      })
    ])
  ) %>%
  filter(length(edge_type) > 0) %>%
  unnest(edge_type) %>%
  ungroup() %>%
  mutate(
    node_id_1 = data_sample$node_id[row_id_1],
    node_id_2 = data_sample$node_id[row_id_2]
  ) %>%
  select(node_id_1, node_id_2, edge_type)
print(edges_list)
nrow(edges_list)
unique(edges_list$edge_type)
# Plotting the Data:
graph <- graph_from_data_frame(d = edges_list, vertices = data_sample, directed = FALSE)
edge_type_colors <- c("payment_type" = "forestgreen", "bank_branch_count_8w" = "blue", "name_email_similarity" = "yellow",
                       "zip_count_4w" = "cyan", "velocity_6h" = "blueviolet", "current_address_months_count" = "gold4",
                       "bank_months_count" = "darkseagreen1", "housing_status" = "darkorange1", "binary_combination_value" = "black")
E(graph)$color <- edge_type_colors[E(graph)$edge_type]
par(mfrow=c(1,1), mar=c(0,0,0,0))
plot(graph, 
     vertex.color = ifelse(V(graph)$fraud_bool, "red", "green"), 
     vertex.size = 5, 
     vertex.label = NA, 
     edge.width = 1, 
     edge.color = E(graph)$color
     )
```



## Sampling Procedure & Justification

Running the entire cleaned and processed dataset post-data cleaning, PCA, and feature extraction would be very resource intensive and not feasible in a local RStudio environment to generate visualizations of the graph. Therefore, we propose a sampling approach that draws a representative sample of the dataset's entire remaining feature space. The methodology for sampling the dataset is explained below.

Due to the lengthy computation time of our edge-generating function, we have chosen to use samples of our full data to analyze the network. A one-thousand node sample takes approximately one minute to generate edges, a two-thousand sample take around ten minutes. This increase in time makes it unfeasible to use larger samples, let alone the entire data set of around 700 thousand nodes. Therefore, we will keep our sample sizes in the 1000 to 2000 range.

The percentage of fraudulent nodes in the full data set is approximately 1%. To ensure this proportion is consistent in our sample we use stratified sampling. Stratified sampling samples 1% of the data from a strata of only fraudulent nodes and the other 99% of the data from a strata of only the non-fraudulent nodes.

Prior to using a sample in our project, we ensure that the sample is representative of the entire data set by applying the Chi-Squared test to our sample. The Chi-Squared test will determine if there is a significant difference between the values in the columns of the sample and in the full data set. The Chi-Squared test generates p-values for each column in a sample. A p-value of less than 0.05 will signal a significant difference between the sample and the full data set for that particular column. By conducting the Chi-Squared test on a given sample we ensure that it is sufficiently representative of the entire data set.



```{r}
# Percentage of fraudulent nodes in the data frame:
# fraud_percentage = ((bankfraud_data$fraud_bool == 1)/nrow(bankfraud_data))*100
# fraud_percentage
# str(data_sample)
# str(binned_bankfraud_data)

# Factor data for chi_Squared test:
data_sample$payment_type = as.factor(data_sample$payment_type)
data_sample$bank_branch_count_8w = as.factor(data_sample$bank_branch_count_8w)
data_sample$zip_count_4w = as.factor(data_sample$zip_count_4w)
data_sample$name_email_similarity = as.factor(data_sample$name_email_similarity)
data_sample$bank_months_count = as.factor(data_sample$bank_months_count)
data_sample$housing_status = as.factor(data_sample$housing_status)
data_sample$velocity_6h = as.factor(data_sample$velocity_6h)
data_sample$current_address_months_count = as.factor(data_sample$current_address_months_count)

binned_bankfraud_data$payment_type = as.factor(binned_bankfraud_data$payment_type)
binned_bankfraud_data$bank_branch_count_8w = as.factor(binned_bankfraud_data$bank_branch_count_8w)
binned_bankfraud_data$zip_count_4w = as.factor(binned_bankfraud_data$zip_count_4w)
binned_bankfraud_data$name_email_similarity = as.factor(binned_bankfraud_data$name_email_similarity)
binned_bankfraud_data$bank_months_count = as.factor(binned_bankfraud_data$bank_months_count)
binned_bankfraud_data$housing_status = as.factor(binned_bankfraud_data$housing_status)
binned_bankfraud_data$velocity_6h = as.factor(binned_bankfraud_data$velocity_6h)
binned_bankfraud_data$current_address_months_count = as.factor(binned_bankfraud_data$current_address_months_count)

# Chi-Squared test:
chi_squared_results = list()

for (col in names(data_sample)[-c(1,3:6,13)]) {
  population_counts = table(binned_bankfraud_data[[col]])
  sample_counts = table(data_sample[[col]])
  
  all_levels = names(population_counts)
  sample_counts = sample_counts[all_levels]
  
  sample_counts[is.na(sample_counts)] = 0
  
  chi_test = chisq.test(x = sample_counts, p = population_counts/sum(population_counts))
  
  chi_squared_results[[col]] = chi_test$p.value
}

chi_squared_df = data.frame(Column = names(chi_squared_results), P_Value = unlist(chi_squared_results))

print(chi_squared_df)
```



## Research Question 1: Which were the key fraudulent players within the network?

This research question aims to determine the most important fraudulent players within the sample taken of the network. Treating each bank account request as a node (row in the dataset), the general process that will be followed will be to observe the distribution of values for each of the following centrality measures, and set a threshold for defining a node as highly influential or not. Using the nodes that influential across different centrality measures, we will then take a look at nodes that have scored high across all centrality metrics and examine them, attempting to interpret their role in facilitating bank fraud based on their scores in the different centrality metrics, proximity to other nodes, and etc.

The following metrics will be used in evaluating the influential fraudulent nodes in the network:

1.  Eigenvector centrality

2.  Degree centrality

3.  Katz centrality

4.  Closeness centrality

5.  PageRank centrality

6.  Degree distribution



```{r}

```



## Research Question 2: Were there any specific fraudulent groups within the network that could be collaborating to defraud the bank? And if so, what were their characteristics?

This research question aims to find clusters of nodes within the entire network, that could potentially be fraud rings based on the attributes of the nodes. Clusters that are majorly the same throughout different results from different network metrics could be potential fraud rings, and will be determined using centrality metrics such as:

1.  Network density

2.  Local clustering coefficient

3.  Modularity

4.  Girvan-Newman algorithm

5.  Graph laplacian

6.  Edge betweenness

7.  Assortativity

8.  Global clustering coefficient

9.  K-cores



```{r}

```



## Research Question 3: What was the average profile of a fraudulent customer?



```{r}

```



## Research Question 4: What differences exist between fraudulent account applications and non-fraudulent account applications?



```{r}

```



## Interpretation Summary

## Conclusion

