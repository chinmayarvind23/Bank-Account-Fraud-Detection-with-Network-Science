[
  {
    "objectID": "code/index.html#sampling-procedure-justification",
    "href": "code/index.html#sampling-procedure-justification",
    "title": "Bank Account Fraud Detection - COSC 421 Course Project",
    "section": "Sampling Procedure & Justification",
    "text": "Sampling Procedure & Justification\nRunning the entire cleaned and processed dataset post-data cleaning, PCA, and feature extraction would be very resource intensive and not feasible in a local RStudio environment to generate visualizations of the graph. Therefore, we propose a sampling approach that draws a representative sample of the dataset’s entire remaining feature space. The methodology for sampling the dataset is explained below.\nDue to the lengthy computation time of our edge-generating function, we have chosen to use samples of our full data to analyze the network. A one-thousand node sample takes approximately one minute to generate edges, a two-thousand sample take around ten minutes. This increase in time makes it unfeasible to use larger samples, let alone the entire data set of around 700 thousand nodes. Therefore, we will keep our sample sizes in the 1000 to 2000 range.\nThe percentage of fraudulent nodes in the full data set is approximately 1%. To ensure this proportion is consistent in our sample we use stratified sampling. Stratified sampling samples 1% of the data from a strata of only fraudulent nodes and the other 99% of the data from a strata of only the non-fraudulent nodes.\nPrior to using a sample in our project, we ensure that the sample is representative of the entire data set by applying the Chi-Squared test to our sample. The Chi-Squared test will determine if there is a significant difference between the values in the columns of the sample and in the full data set. The Chi-Squared test generates p-values for each column in a sample. A p-value of less than 0.05 will signal a significant difference between the sample and the full data set for that particular column. By conducting the Chi-Squared test on a given sample we ensure that it is sufficiently representative of the entire data set.\n\n# Percentage of fraudulent nodes in the data frame:\n# fraud_percentage = ((bankfraud_data$fraud_bool == 1)/nrow(bankfraud_data))*100\n# fraud_percentage\n# str(data_sample)\n# str(binned_bankfraud_data)\n\n# Factor data for chi_Squared test:\ndata_sample$payment_type = as.factor(data_sample$payment_type)\ndata_sample$bank_branch_count_8w = as.factor(data_sample$bank_branch_count_8w)\ndata_sample$zip_count_4w = as.factor(data_sample$zip_count_4w)\ndata_sample$name_email_similarity = as.factor(data_sample$name_email_similarity)\ndata_sample$bank_months_count = as.factor(data_sample$bank_months_count)\ndata_sample$housing_status = as.factor(data_sample$housing_status)\ndata_sample$velocity_6h = as.factor(data_sample$velocity_6h)\ndata_sample$current_address_months_count = as.factor(data_sample$current_address_months_count)\n\nbinned_bankfraud_data$payment_type = as.factor(binned_bankfraud_data$payment_type)\nbinned_bankfraud_data$bank_branch_count_8w = as.factor(binned_bankfraud_data$bank_branch_count_8w)\nbinned_bankfraud_data$zip_count_4w = as.factor(binned_bankfraud_data$zip_count_4w)\nbinned_bankfraud_data$name_email_similarity = as.factor(binned_bankfraud_data$name_email_similarity)\nbinned_bankfraud_data$bank_months_count = as.factor(binned_bankfraud_data$bank_months_count)\nbinned_bankfraud_data$housing_status = as.factor(binned_bankfraud_data$housing_status)\nbinned_bankfraud_data$velocity_6h = as.factor(binned_bankfraud_data$velocity_6h)\nbinned_bankfraud_data$current_address_months_count = as.factor(binned_bankfraud_data$current_address_months_count)\n\n# Chi-Squared test:\nchi_squared_results = list()\n\nfor (col in names(data_sample)[-c(1,3:6,13)]) {\n  population_counts = table(binned_bankfraud_data[[col]])\n  sample_counts = table(data_sample[[col]])\n  \n  all_levels = names(population_counts)\n  sample_counts = sample_counts[all_levels]\n  \n  sample_counts[is.na(sample_counts)] = 0\n  \n  chi_test = chisq.test(x = sample_counts, p = population_counts/sum(population_counts))\n  \n  chi_squared_results[[col]] = chi_test$p.value\n}\n\nWarning in chisq.test(x = sample_counts, p =\npopulation_counts/sum(population_counts)): Chi-squared approximation may be\nincorrect\nWarning in chisq.test(x = sample_counts, p =\npopulation_counts/sum(population_counts)): Chi-squared approximation may be\nincorrect\nWarning in chisq.test(x = sample_counts, p =\npopulation_counts/sum(population_counts)): Chi-squared approximation may be\nincorrect\nWarning in chisq.test(x = sample_counts, p =\npopulation_counts/sum(population_counts)): Chi-squared approximation may be\nincorrect\nWarning in chisq.test(x = sample_counts, p =\npopulation_counts/sum(population_counts)): Chi-squared approximation may be\nincorrect\nWarning in chisq.test(x = sample_counts, p =\npopulation_counts/sum(population_counts)): Chi-squared approximation may be\nincorrect\nWarning in chisq.test(x = sample_counts, p =\npopulation_counts/sum(population_counts)): Chi-squared approximation may be\nincorrect\nWarning in chisq.test(x = sample_counts, p =\npopulation_counts/sum(population_counts)): Chi-squared approximation may be\nincorrect\n\nchi_squared_df = data.frame(Column = names(chi_squared_results), P_Value = unlist(chi_squared_results))\n\nprint(chi_squared_df)\n\n                                                   Column   P_Value\npayment_type                                 payment_type 0.1909177\nbank_branch_count_8w                 bank_branch_count_8w 0.1618741\nzip_count_4w                                 zip_count_4w 0.6715799\nname_email_similarity               name_email_similarity 0.5884381\nbank_months_count                       bank_months_count 0.2614375\nhousing_status                             housing_status 0.9756855\nvelocity_6h                                   velocity_6h 0.6176175\ncurrent_address_months_count current_address_months_count 0.7021067\nbinary_combination_value         binary_combination_value 0.9681636"
  },
  {
    "objectID": "code/index.html#data-description",
    "href": "code/index.html#data-description",
    "title": "Bank Account Fraud Detection - COSC 421 Course Project",
    "section": "Data Description",
    "text": "Data Description\nThe dataset post-processing has been reduced to the following columns that will be considered in the following research questions:\n\nnode_id - The ID of the node\npayment_type - The different types of credit payment plans available\nkeep_alive_session - Binary variable to represent if a user is still browsing post logging out of the bank’s website\nforeign_request - If the bank account request came from a different country than the bank’s country\nemail_is_free - If the user’s email is a business or non-business email\nfraud_bool - Fraudulent status of the bank account request\nbank_branch_count_8w - Number of bank account requests in the last 2 months for the branch that the request went to\nzip_count_4w - Number of bank account applications within the same zip code as a particular application\nname_email_similarity - Similarity of user’s name to their email address\nbank_months_count - Number of months a user has held a previous bank account for\nhousing_status - Type of housing a user is living in\nvelocity_6h - Number of bank account requests created in the last 6 hours\nphone_home_valid - If a user’s provided phone number is valid or not\ncurrent_address_months_count - How long in months a user has lived at their current address"
  },
  {
    "objectID": "code/index.html#research-question-1-which-were-the-key-fraudulent-players-within-the-network",
    "href": "code/index.html#research-question-1-which-were-the-key-fraudulent-players-within-the-network",
    "title": "Bank Account Fraud Detection - COSC 421 Course Project",
    "section": "Research Question 1: Which were the key fraudulent players within the network?",
    "text": "Research Question 1: Which were the key fraudulent players within the network?\nThis research question aims to determine the most important fraudulent players within the sample taken of the network. Treating each bank account request as a node (row in the dataset), the general process that will be followed will be to observe the distribution of values for each of the following centrality measures, and set a threshold for defining a node as highly influential or not, and make a recommendation on which nodes should be further analyzed through domain expertise, and other ground truth information to determine fraud rings and/or individuals that were involved in defrauding the bank either directly or indirectly.\nThe overarching hypothesis of this project is that the links between the nodes due to the similarities in the attributes of their bank account requests could possibly suggest that some nodes were involved in defrauding the bank in groups, or may have been part of the crime itself in isolation, or were involved in some manner in defrauding the bank, and the research questions aim to figure out those connections between the nodes, and whether it makes practical sense to flag the identified nodes as suspicious.\n\nbankfraud_data\n\n# A tibble: 743,169 × 14\n   node_id payment_type keep_alive_session foreign_request email_is_free\n     &lt;int&gt;        &lt;dbl&gt;              &lt;dbl&gt;           &lt;dbl&gt;         &lt;dbl&gt;\n 1       1            1                  1               0             1\n 2       2            4                  1               0             1\n 3       3            2                  0               0             1\n 4       4            2                  1               0             1\n 5       5            1                  0               0             0\n 6       6            4                  1               0             1\n 7       7            2                  1               0             1\n 8       8            2                  1               0             0\n 9       9            2                  1               0             0\n10      10            4                  1               0             1\n# ℹ 743,159 more rows\n# ℹ 9 more variables: fraud_bool &lt;dbl&gt;, bank_branch_count_8w &lt;dbl&gt;,\n#   zip_count_4w &lt;dbl&gt;, name_email_similarity &lt;dbl&gt;, bank_months_count &lt;dbl&gt;,\n#   housing_status &lt;dbl&gt;, velocity_6h &lt;dbl&gt;, phone_home_valid &lt;dbl&gt;,\n#   current_address_months_count &lt;dbl&gt;\n\n\nThe following metrics will be used in identifying the influential fraudulent nodes in the network:\n\nEigenvector centrality\nBetweenness centrality\nPageRank centrality\n\nBelow is the sample creation and plotting process, the sample drawn is random, and is representative of the dataset. The process below can be applied to the entire dataset (resource-permitting), but since we do not have the resources to generate the entire graph, we will be working with samples in all of the research questions.\n\n# Creating sample\nsample_size = 50\ndata_sample_q1 = sample_n(binned_bankfraud_data, sample_size)\nstr(data_sample_q1)\n\ntibble [50 × 15] (S3: tbl_df/tbl/data.frame)\n $ node_id                     : int [1:50] 581209 157405 180143 559747 724460 92751 334311 492059 215087 566939 ...\n $ payment_type                : Factor w/ 5 levels \"1\",\"2\",\"3\",\"4\",..: 2 1 4 1 1 1 2 2 1 2 ...\n $ keep_alive_session          : num [1:50] 1 1 0 1 0 0 0 0 1 1 ...\n $ foreign_request             : num [1:50] 0 0 0 0 0 0 0 0 0 0 ...\n $ email_is_free               : num [1:50] 1 0 1 1 0 0 1 0 0 1 ...\n $ fraud_bool                  : num [1:50] 0 0 0 0 0 0 0 0 0 0 ...\n $ bank_branch_count_8w        : Factor w/ 5 levels \"1\",\"2\",\"3\",\"4\",..: 3 1 1 1 1 4 2 1 1 1 ...\n $ zip_count_4w                : Factor w/ 5 levels \"1\",\"2\",\"3\",\"4\",..: 2 1 2 2 1 1 1 3 1 1 ...\n $ name_email_similarity       : Factor w/ 5 levels \"1\",\"2\",\"3\",\"4\",..: 5 4 5 1 3 2 5 2 2 2 ...\n $ bank_months_count           : Factor w/ 5 levels \"1\",\"2\",\"3\",\"4\",..: 1 5 5 2 1 3 5 1 1 1 ...\n $ housing_status              : Factor w/ 7 levels \"1\",\"2\",\"3\",\"4\",..: 3 5 3 2 3 3 2 5 1 3 ...\n $ velocity_6h                 : Factor w/ 5 levels \"1\",\"2\",\"3\",\"4\",..: 1 2 3 1 2 4 3 1 4 1 ...\n $ phone_home_valid            : num [1:50] 1 1 0 1 1 0 0 0 0 1 ...\n $ current_address_months_count: Factor w/ 5 levels \"1\",\"2\",\"3\",\"4\",..: 1 1 1 1 1 1 1 4 1 5 ...\n $ binary_combination_value    : chr [1:50] \"10101\" \"10001\" \"00100\" \"10101\" ...\n\n# Creating edge list\nedges_list &lt;- expand.grid(row_id_1 = seq_len(nrow(data_sample_q1)), row_id_2 = seq_len(nrow(data_sample_q1))) %&gt;%\n  filter(row_id_1 &lt; row_id_2) %&gt;% \n  rowwise() %&gt;%\n  mutate(\n    edge_type = list(names(data_sample_q1)[-c(1,3:6,13)][\n      sapply(names(data_sample_q1)[-c(1,3:6,13)], function(col) {\n        data_sample_q1[[col]][row_id_1] == data_sample_q1[[col]][row_id_2]\n      })\n    ])\n  ) %&gt;%\n  filter(length(edge_type) &gt; 0) %&gt;%\n  unnest(edge_type) %&gt;%\n  ungroup() %&gt;%\n  mutate(\n    node_id_1 = data_sample_q1$node_id[row_id_1],\n    node_id_2 = data_sample_q1$node_id[row_id_2]\n  ) %&gt;%\n  select(node_id_1, node_id_2, edge_type)\nprint(edges_list)\n\n# A tibble: 3,682 × 3\n   node_id_1 node_id_2 edge_type                   \n       &lt;int&gt;     &lt;int&gt; &lt;chr&gt;                       \n 1    581209    157405 current_address_months_count\n 2    581209    180143 zip_count_4w                \n 3    581209    180143 name_email_similarity       \n 4    581209    180143 housing_status              \n 5    581209    180143 current_address_months_count\n 6    157405    180143 bank_branch_count_8w        \n 7    157405    180143 bank_months_count           \n 8    157405    180143 current_address_months_count\n 9    581209    559747 zip_count_4w                \n10    581209    559747 velocity_6h                 \n# ℹ 3,672 more rows\n\nnrow(edges_list)\n\n[1] 3682\n\nunique(edges_list$edge_type)\n\n[1] \"current_address_months_count\" \"zip_count_4w\"                \n[3] \"name_email_similarity\"        \"housing_status\"              \n[5] \"bank_branch_count_8w\"         \"bank_months_count\"           \n[7] \"velocity_6h\"                  \"binary_combination_value\"    \n[9] \"payment_type\"                \n\n# Plotting the Graph\ngraph_rq1 &lt;- graph_from_data_frame(d = edges_list, vertices = data_sample_q1, directed = FALSE)\nedge_type_colors &lt;- c(\"payment_type\" = \"green\", \"bank_branch_count_8w\" = \"blue\", \"name_email_similarity\" = \"yellow\",\n                       \"zip_count_4w\" = \"cyan\", \"velocity_6h\" = \"blueviolet\", \"current_address_months_count\" = \"gold4\",\n                       \"bank_months_count\" = \"darkseagreen1\", \"housing_status\" = \"darkorange1\", \"binary_combination_value\" = \"black\")\nE(graph_rq1)$color &lt;- edge_type_colors[E(graph_rq1)$edge_type]\npar(mfrow=c(1,1), mar=c(0,0,0,0))\nplot(graph_rq1, vertex.size = 20, edge.width = 1, edge.color = E(graph)$color)\n\n\n\n\n\n\n\n\nNow, we will construct the columns for the different centrality metrics stated above, and interpret the metrics in the context of bank fraud detection. We will start with eigenvector centrality. All the nodes within the sample are considered when determining the nodes that are influential based on the eigenvector centrality scores of the nodes.\n\ndata_sample_q1\n\n# A tibble: 50 × 15\n   node_id payment_type keep_alive_session foreign_request email_is_free\n     &lt;int&gt; &lt;fct&gt;                     &lt;dbl&gt;           &lt;dbl&gt;         &lt;dbl&gt;\n 1  581209 2                             1               0             1\n 2  157405 1                             1               0             0\n 3  180143 4                             0               0             1\n 4  559747 1                             1               0             1\n 5  724460 1                             0               0             0\n 6   92751 1                             0               0             0\n 7  334311 2                             0               0             1\n 8  492059 2                             0               0             0\n 9  215087 1                             1               0             0\n10  566939 2                             1               0             1\n# ℹ 40 more rows\n# ℹ 10 more variables: fraud_bool &lt;dbl&gt;, bank_branch_count_8w &lt;fct&gt;,\n#   zip_count_4w &lt;fct&gt;, name_email_similarity &lt;fct&gt;, bank_months_count &lt;fct&gt;,\n#   housing_status &lt;fct&gt;, velocity_6h &lt;fct&gt;, phone_home_valid &lt;dbl&gt;,\n#   current_address_months_count &lt;fct&gt;, binary_combination_value &lt;chr&gt;\n\n\n\n# Computing the eigenvector centrality scores and adding them to the data sample\neigencentralities &lt;- eigen_centrality(graph_rq1)\ndata_sample_q1[, 16] &lt;- eigencentralities$vector\ncolnames(data_sample_q1)[16] &lt;- \"eigen_vector_centrality_score\"\n\n# Ordering the nodes based on descending eigenvector centrality scores to determine the nodes with the highest relative eigenvector centrality scores\ndata_sample_q1_ordered_ev &lt;- arrange(data_sample_q1, desc(data_sample_q1$`eigen_vector_centrality_score`))\n\n# Extracting the records in the top 25 percentiles of the eigenvector centrality scores to pick out the most important nodes from the sample\neigenvector_centrality_threshold &lt;- quantile(data_sample_q1_ordered_ev$eigen_vector_centrality_score, 0.75)\nmost_influential_nodes_by_ev_centrality &lt;- data_sample_q1_ordered_ev[data_sample_q1_ordered_ev$eigen_vector_centrality_score &gt; eigenvector_centrality_threshold, ]\nmost_influential_nodes_by_ev_centrality\n\n# A tibble: 13 × 16\n   node_id payment_type keep_alive_session foreign_request email_is_free\n     &lt;int&gt; &lt;fct&gt;                     &lt;dbl&gt;           &lt;dbl&gt;         &lt;dbl&gt;\n 1  462862 2                             1               0             1\n 2  234179 2                             0               0             1\n 3  216154 2                             0               0             1\n 4  241299 2                             0               0             0\n 5  157405 1                             1               0             0\n 6  724460 1                             0               0             0\n 7   78924 1                             1               0             0\n 8  372515 2                             1               0             0\n 9  277272 2                             1               0             1\n10  282853 1                             0               0             0\n11  663037 2                             1               0             1\n12  582014 1                             1               0             0\n13  521344 2                             0               0             0\n# ℹ 11 more variables: fraud_bool &lt;dbl&gt;, bank_branch_count_8w &lt;fct&gt;,\n#   zip_count_4w &lt;fct&gt;, name_email_similarity &lt;fct&gt;, bank_months_count &lt;fct&gt;,\n#   housing_status &lt;fct&gt;, velocity_6h &lt;fct&gt;, phone_home_valid &lt;dbl&gt;,\n#   current_address_months_count &lt;fct&gt;, binary_combination_value &lt;chr&gt;,\n#   eigen_vector_centrality_score &lt;dbl&gt;\n\n\nInfluential/important nodes in the context of bank account fraud detection are nodes that are possibly committing bank account fraud, and/or could be in a group that is defrauding the bank.\nThe threshold of the top 25 percentiles was set in order to detect only the most important nodes with a high value for the eigenvector centrality, and which may not themselves be fraudulent but can be flagged as suspicious based on the quality of their connections within the network, and could possibly be collaborating with other nodes in the network to defraud the bank. Eigenvector centrality scores in the context of bank account fraud detection tell us that the higher the eigenvector centrality score for a node, the higher the number of important nodes that it is connected to.\nThe nodes that appear above the threshold for the calculated metrics can be determined with confidence as suspicious, but the others can be further investigated due to their ties to various nodes that were flagged.\n13 nodes were flagged as suspicious from this sample based on them exceeding the threshold of the top 25 percentiles for the eigenvector centrality score, and in the context of bank fraud, they seem to have connections to node(s) that have been caught committing bank fraud or may have been caught committing bank fraud themselves.The IDs of the nodes that were flagged based on their high eigenvector centrality were as follows:\n\nnodes_flagged_ids_eigenvector &lt;- most_influential_nodes_by_ev_centrality[, 1]\nprint(as.list(nodes_flagged_ids_eigenvector))\n\n$node_id\n [1] 462862 234179 216154 241299 157405 724460  78924 372515 277272 282853\n[11] 663037 582014 521344\n\nnodes_flagged_ids_eigenvector &lt;- as.vector(nodes_flagged_ids_eigenvector)\n\nNodes 538450, 503729, 253211, 503830, 562008, 464472, 249217, 683809, 290984, 375304, 690415, 597139, and 80189 were deemed as suspicious from the above drawn sample.\nAll the columns for the nodes that were flagged suspicious can be further investigated by domain experts to make the best judgement on whether the flagged nodes were participating in bank fraud or were part of any fraud rings, or whether some of the nodes flagged were false positives.\nThe main advantage of using eigenvector centrality as a metric to flag nodes as suspicious is that eigenvector centrality takes into account both the quantity and the quality of the connections, and the main disadvantage of Eigenvector centrality is that large hubs can influence centrality scores of other nodes, and could ruin the quality of the centrality scores assigned.\nNow, we will calculate the betweenness centrality scores for the above drawn sample of nodes, and flag suspicious nodes in a similar manner as performed above to identify potentially influential nodes in the context of committing bank account fraud.\n\n# Computing the betweenness centrality scores of the nodes in the sample\nbetweenness_centrality_scores &lt;- betweenness(graph_rq1)\ndata_sample_q1[, 17] &lt;- betweenness_centrality_scores\ncolnames(data_sample_q1)[17] &lt;- \"betweenness_centrality_score\"\n\n# Ordering the nodes based on descending betweenness centrality scores to determine the nodes with the highest relative betweenness centrality scores\ndata_sample_q1_ordered_betweenness &lt;- arrange(data_sample_q1, desc(data_sample_q1$betweenness_centrality_score))\n\n# Extracting the records in the top 25 percentiles of the betweenness centrality scores to pick out the most important nodes from the sample\nbetweenness_centrality_threshold &lt;- quantile(data_sample_q1_ordered_betweenness$betweenness_centrality_score, 0.75)\nmost_influential_nodes_by_betweenness_centrality &lt;- data_sample_q1_ordered_betweenness[data_sample_q1_ordered_betweenness$betweenness_centrality_score &gt; betweenness_centrality_threshold, ]\nmost_influential_nodes_by_betweenness_centrality\n\n# A tibble: 13 × 17\n   node_id payment_type keep_alive_session foreign_request email_is_free\n     &lt;int&gt; &lt;fct&gt;                     &lt;dbl&gt;           &lt;dbl&gt;         &lt;dbl&gt;\n 1  216154 2                             0               0             1\n 2  435058 1                             0               0             1\n 3  180143 4                             0               0             1\n 4  646377 4                             1               0             1\n 5  481867 2                             0               0             1\n 6  334311 2                             0               0             1\n 7  206850 2                             1               0             1\n 8  216969 1                             1               0             1\n 9  724460 1                             0               0             0\n10  559747 1                             1               0             1\n11   78924 1                             1               0             0\n12  277272 2                             1               0             1\n13  282853 1                             0               0             0\n# ℹ 12 more variables: fraud_bool &lt;dbl&gt;, bank_branch_count_8w &lt;fct&gt;,\n#   zip_count_4w &lt;fct&gt;, name_email_similarity &lt;fct&gt;, bank_months_count &lt;fct&gt;,\n#   housing_status &lt;fct&gt;, velocity_6h &lt;fct&gt;, phone_home_valid &lt;dbl&gt;,\n#   current_address_months_count &lt;fct&gt;, binary_combination_value &lt;chr&gt;,\n#   eigen_vector_centrality_score &lt;dbl&gt;, betweenness_centrality_score &lt;dbl&gt;\n\n\nNodes with high betweenness centrality scores are nodes that tend to be the middlemen for information flow, and in the context of bank fraud, these nodes can be vital to the fraud rings as middlemen for information flow about defrauding the bank. Removing these nodes from the network could collapse the entire fraud ring, and it would be in the bank’s best interest to flag these nodes, and further investigate the attributes of these nodes to deem them as being part of a fraudulent group or being fraudulent.\nThe threshold of the top 25 percentiles was set in order to detect only the most important nodes with a high value for the betweenness centrality, and which may not themselves be fraudulent but can be flagged as suspicious based on their ability to act as middlemen for information flow within the network, and could possibly be collaborating with other nodes in the network to defraud the bank or be the fraud ring leaders themselves due to them being on the shortest paths to many nodes within the network.\n13 nodes were flagged using the betweenness centrality approach to detecting bank account fraud and can be further investigated by domain experts to identify whether the nodes flagged were part of fraud rings or were fraudulent. The IDs of the nodes that were flagged based on their high betweenness centrality were as follows:\n\nnodes_flagged_ids_betweenness &lt;- most_influential_nodes_by_betweenness_centrality[, 1]\nprint(as.list(nodes_flagged_ids_betweenness))\n\n$node_id\n [1] 216154 435058 180143 646377 481867 334311 206850 216969 724460 559747\n[11]  78924 277272 282853\n\nnodes_flagged_ids_betweenness &lt;- as.vector(nodes_flagged_ids_betweenness)\n\nNodes 690415, 195760, 569089, 250443, 45412, 503729, 392700, 503830, 655584, 683809, 375304, 538450, and 142900 were deemed as suspicious from the above drawn sample.\nThe main advantage of using betweenness centrality as a metric to flag nodes as suspicious is that betweenness centrality identifies the nodes that act as middlemen to the most number of nodes, and if removed can cause the information flow to be disrupted severely. The main disadvantage of using betweenness centrality is that it can be computationally expensive due to the metric needing to calculate a lot of shortest paths.\nNext, we will calculate the PageRank centrality scores for the above drawn sample of nodes, and flag suspicious nodes in a similar manner as performed above to identify potentially influential nodes in the context of committing bank account fraud.\n\n# Computing the PageRank centrality scores of the nodes in the sample\npagerank_centrality_scores &lt;- page_rank(graph_rq1)\ndata_sample_q1[, 18] &lt;- pagerank_centrality_scores$vector\ncolnames(data_sample_q1)[18] &lt;- \"pagerank_centrality_score\"\n\n# Ordering the nodes based on descending pagerank centrality scores to determine the nodes with the highest relative pagerank centrality scores\ndata_sample_q1_ordered_pagerank &lt;- arrange(data_sample_q1, desc(data_sample_q1$pagerank_centrality_score))\n\n# Extracting the records in the top 25 percentiles of the pagerank centrality scores to pick out the most important nodes from the sample\npagerank_centrality_threshold &lt;- quantile(data_sample_q1_ordered_pagerank$pagerank_centrality_score, 0.75)\nmost_influential_nodes_by_pagerank_centrality &lt;- data_sample_q1_ordered_pagerank[data_sample_q1_ordered_pagerank$pagerank_centrality_score &gt; pagerank_centrality_threshold, ]\nmost_influential_nodes_by_pagerank_centrality\n\n# A tibble: 13 × 18\n   node_id payment_type keep_alive_session foreign_request email_is_free\n     &lt;int&gt; &lt;fct&gt;                     &lt;dbl&gt;           &lt;dbl&gt;         &lt;dbl&gt;\n 1  462862 2                             1               0             1\n 2  234179 2                             0               0             1\n 3  216154 2                             0               0             1\n 4  241299 2                             0               0             0\n 5  724460 1                             0               0             0\n 6   78924 1                             1               0             0\n 7  282853 1                             0               0             0\n 8  372515 2                             1               0             0\n 9  157405 1                             1               0             0\n10  277272 2                             1               0             1\n11  663037 2                             1               0             1\n12  582014 1                             1               0             0\n13  521344 2                             0               0             0\n# ℹ 13 more variables: fraud_bool &lt;dbl&gt;, bank_branch_count_8w &lt;fct&gt;,\n#   zip_count_4w &lt;fct&gt;, name_email_similarity &lt;fct&gt;, bank_months_count &lt;fct&gt;,\n#   housing_status &lt;fct&gt;, velocity_6h &lt;fct&gt;, phone_home_valid &lt;dbl&gt;,\n#   current_address_months_count &lt;fct&gt;, binary_combination_value &lt;chr&gt;,\n#   eigen_vector_centrality_score &lt;dbl&gt;, betweenness_centrality_score &lt;dbl&gt;,\n#   pagerank_centrality_score &lt;dbl&gt;\n\n\nNodes with high pagerank centrality scores are nodes that are connected to other nodes with other nodes with high pagerank centrality, and this centrality metric takes into account the number of links, the quality of the links, and the quality of the nodes that a node is connected to. In the context of bank fraud, these nodes are nodes that are possibly committing bank account fraud, and/or could be in a group that is defrauding the bank.\nThe threshold of the top 25 percentiles was set in order to detect only the most important nodes with a high value for the pagerank centrality, and which may not themselves be fraudulent but can be flagged as suspicious based on their ability to be central members in fraud rings or maybe committing fraud in isolation. Pagerank centrality scores in the context of bank account fraud detection tell us that the higher the pagerank centrality score for a node, the higher the number of important nodes that it is connected to in a standardized manner by dividing the centrality values by the outdegree in the formula for calculating pagerank centrality.\n13 nodes were flagged using the pagerank centrality approach to detecting bank account fraud and can be further investigated by domain experts to identify whether the nodes flagged were part of fraud rings or were fraudulent. The IDs of the nodes that were flagged based on their high pagerank centrality were as follows:\n\nnodes_flagged_ids_pagerank &lt;- most_influential_nodes_by_pagerank_centrality[, 1]\nprint(as.list(nodes_flagged_ids_pagerank))\n\n$node_id\n [1] 462862 234179 216154 241299 724460  78924 282853 372515 157405 277272\n[11] 663037 582014 521344\n\nnodes_flagged_ids_pagerank &lt;- as.vector(nodes_flagged_ids_pagerank)\n\nNodes 538450, 503729, 253211, 503830, 562008, 464472, 249217, 683809, 290984, 690415, 375304, 569089, and 195760 were deemed as suspicious from the above drawn sample.\nThe main disadvantage of using pagerank centrality is that it is prone to link manipulation, which is when many low quality links are accumulated, it can cause the pagerank of a page to be good even though the pages linking to it did not have a high pagerank score, leading to nodes being falsely classified as important, and it also does not take into account the fraud_bool variable from the dataset when defining nodes as more important as others from the drawn sample.\nThe following metrics are not suitable for identifying the influential fraudulent nodes in the network:\n\nKatz centrality\n\nAs this graph was a heavily connected undirected multigraph, there are barely any if not 0 nodes with an indegree of 0, and the advantage of using Katz centrality is that it accommodates for nodes with an indegree of 0 by providing them with “free” centrality scores in addition to the computed centrality scores.\n\nCloseness centrality\n\nIn the context of how we have defined our graph as containing links between users based on the similarity of their bank account requests, closeness centrality does not make much sense as a centrality metric due to it being about the speed at which information can reach other nodes from a starting node, and not about whether the node is accessible to many nodes like in the case of betweenness centrality, therefore, it is not appropriate in our context.\n\nDegree centrality\n\nIn the context of how we have defined our graph as containing links between users based on the similarity of their bank account requests, degree centrality is not an appropriate centrality metric due to it focusing purely on the number of nodes a particular node is connected to, and is not concerned about the centrality of the nodes that a particular node is connected to, which may lead to incorrectly deeming a node as a suspicious, when it may not be.\nBelow is a venn diagram of the nodes that were flagged as suspicious by various metrics, and which nodes are deemed by more than 1 centrality metric as being suspicious which can be used in further domain-specific analysis to determine whether the nodes are linked to fradulent entities or whose request may be frauduelent:\n\n#install.packages(\"VennDiagram\")\nlibrary(grid)\nlibrary(VennDiagram)\n\nLoading required package: futile.logger\n\nvenndiagram &lt;- venn.diagram(x = list(nodes_flagged_ids_betweenness$node_id, nodes_flagged_ids_eigenvector$node_id, nodes_flagged_ids_pagerank$node_id), category.names = c(\"Betweenness Centrality\", \"Eigenvector Centrality\", \"PageRank Centrality\"), filename = NULL, main = \"Venn Diagram of the number of Overlapping Flagged Nodes from the different centrality metrics\", output = TRUE, fill = c(\"red\", \"green\", \"blue\"), alpha = 0.5, cex = 0.75, cat.cex = 0.75)\ngrid.newpage()\ngrid.draw(venndiagram)\n\n\n\n\n\n\n\n\nThe red colored circle represents the number of Node IDs of the high betweenness centrality flagged nodes, the green colored circle represents the number of Node IDs of the high eigenvector centrality flagged nodes, and the blue colored circle represents the number of Node IDs of the high pagerank centrality flagged nodes. The nodes that were flagged as having high values for all 3 centrality metrics were nodes 690415, 503729, 503830, 683809, 375304, and 538450.\n\nmost_suspicious_nodes_by_all_metrics &lt;- intersect(intersect(nodes_flagged_ids_betweenness$node_id, nodes_flagged_ids_eigenvector$node_id), nodes_flagged_ids_pagerank$node_id)\nprint(most_suspicious_nodes_by_all_metrics)\n\n[1] 216154 724460  78924 277272 282853\n\n\nThe above 6 nodes would be the best to investigate further if investigating all 39 nodes flagged as suspicious is infeasible."
  },
  {
    "objectID": "code/index.html#research-question-2-what-was-the-average-profile-of-a-fraudulent-customer",
    "href": "code/index.html#research-question-2-what-was-the-average-profile-of-a-fraudulent-customer",
    "title": "Bank Account Fraud Detection - COSC 421 Course Project",
    "section": "Research Question 2: What was the average profile of a fraudulent customer?",
    "text": "Research Question 2: What was the average profile of a fraudulent customer?"
  },
  {
    "objectID": "code/index.html#research-question-3-were-there-any-specific-fraudulent-groups-within-the-network-that-could-be-collaborating-to-defraud-the-bank-and-if-so-what-were-their-characteristics",
    "href": "code/index.html#research-question-3-were-there-any-specific-fraudulent-groups-within-the-network-that-could-be-collaborating-to-defraud-the-bank-and-if-so-what-were-their-characteristics",
    "title": "Bank Account Fraud Detection - COSC 421 Course Project",
    "section": "Research Question 3: Were there any specific fraudulent groups within the network that could be collaborating to defraud the bank? And if so, what were their characteristics?",
    "text": "Research Question 3: Were there any specific fraudulent groups within the network that could be collaborating to defraud the bank? And if so, what were their characteristics?\nThis research question aims to find clusters of nodes within the entire network, that could potentially be fraud rings based on the attributes of the nodes. Clusters that are majorly the same throughout different results from different network metrics could be potential fraud rings, and will be determined using centrality metrics such as:\n\nNetwork density\nLocal clustering coefficient\nModularity\nGraph laplacian\nEdge betweenness\nGlobal clustering coefficient\nK-cores"
  },
  {
    "objectID": "code/index.html#research-question-4-what-differences-exist-between-fraudulent-account-applications-and-non-fraudulent-account-applications",
    "href": "code/index.html#research-question-4-what-differences-exist-between-fraudulent-account-applications-and-non-fraudulent-account-applications",
    "title": "Bank Account Fraud Detection - COSC 421 Course Project",
    "section": "Research Question 4: What differences exist between fraudulent account applications and non-fraudulent account applications?",
    "text": "Research Question 4: What differences exist between fraudulent account applications and non-fraudulent account applications?\nQUARTO RENDER"
  },
  {
    "objectID": "code/index.html#interpretation-summary",
    "href": "code/index.html#interpretation-summary",
    "title": "Bank Account Fraud Detection - COSC 421 Course Project",
    "section": "Interpretation Summary",
    "text": "Interpretation Summary"
  },
  {
    "objectID": "code/index.html#conclusion",
    "href": "code/index.html#conclusion",
    "title": "Bank Account Fraud Detection - COSC 421 Course Project",
    "section": "Conclusion",
    "text": "Conclusion"
  }
]